{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f1132d-2a9d-43a9-9a5e-cc457e3de8d5",
   "metadata": {},
   "source": [
    "## Step 0 – Imports and helper functions  \n",
    "This section loads core scientific libraries (`pandas`, `numpy`, `pathlib`) and defines constants used throughout the data-processing pipeline.  \n",
    "It includes:  \n",
    "- The 95 % z-score for Wilson confidence intervals (`Z_95`)  \n",
    "- A dictionary converting month names to numbers (`MONTH_MAP`)  \n",
    "- A vectorized `wilson_ci()` function that computes the binomial proportion and 95 % confidence interval for survey detections.  \n",
    "These utilities are reused later to calculate occupancy estimates for *Zostera capricorni* and *Halophila ovalis*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "153e793d-514c-46ee-a8b5-a9e192bdb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 — Imports & helpers\n",
    "# If needed (run in a terminal/Anaconda prompt first):\n",
    "# pip install pandas numpy matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 95% z-score (for Wilson CI)\n",
    "Z_95 = 1.959963984540054\n",
    "\n",
    "# Month name → number (only used if MONTH is text like \"October\")\n",
    "MONTH_MAP = {m.lower(): i for i, m in enumerate(\n",
    "    [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"], start=1)}\n",
    "\n",
    "def wilson_ci(k, n, z=Z_95):\n",
    "    \"\"\"Vectorized Wilson 95% CI for binomial proportion.\"\"\"\n",
    "    k = np.asarray(k, dtype=float)\n",
    "    n = np.asarray(n, dtype=float)\n",
    "    p = np.divide(k, n, out=np.full_like(k, np.nan, dtype=float), where=n>0)\n",
    "    denom = 1 + z**2/n\n",
    "    center = (p + z**2/(2*n)) / denom\n",
    "    half = (z / denom) * np.sqrt((p*(1-p)/n) + (z**2/(4*n**2)))\n",
    "    return p, center - half, center + half"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493cd628-f652-46cb-abd2-b47e0395b3cf",
   "metadata": {},
   "source": [
    "## Step 1 – File paths and data import  \n",
    "This block defines the working directory and helper functions for locating and reading input files.  \n",
    "It performs three main tasks:  \n",
    "1. Uses `pick()` to automatically find the biology, environment, and site-location files based on name patterns.  \n",
    "2. Implements `read_any()` to load either CSV/TXT or Excel files using the appropriate pandas reader.  \n",
    "3. Verifies successful loading by printing file names and dataset dimensions for `bio`, `env`, and `sites_df`.  \n",
    "This ensures all required inputs exist and are ready for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55509571-27f0-402d-9f60-9254c842e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using files:\n",
      " - GBR_NESP-TWQ-3.2.1-5.4_JCU_Seagrass_1984-2018_Site-surveys.csv \n",
      " - 2509.28d55d4-collected.csv \n",
      " - seagrass location.csv\n",
      "Loaded → bio: (81387, 24) | env: (5600, 12) | sites: (8, 3)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "RAW = Path(\".\")         \n",
    "\n",
    "if not RAW.exists():\n",
    "    raise FileNotFoundError(f\"{RAW.resolve()}\")\n",
    "\n",
    "\n",
    "def pick(pattern: str) -> Path:\n",
    "    hits = sorted(RAW.glob(pattern))\n",
    "    if not hits:\n",
    "        raise FileNotFoundError(f\" {RAW} {pattern}\")\n",
    "    return hits[0]\n",
    "\n",
    "BIO_PATH   = pick(\"GBR_NESP-TWQ-3.2.1-5.4_JCU_Seagr*.*\")\n",
    "ENV_PATH   = pick(\"2509.28d55d4-collected*.*\")\n",
    "SITES_PATH = pick(\"seagrass location*.*\")\n",
    "\n",
    "print(\"Using files:\\n -\", BIO_PATH.name, \"\\n -\", ENV_PATH.name, \"\\n -\", SITES_PATH.name)\n",
    "\n",
    "\n",
    "def read_any(p: Path):\n",
    "    ext = p.suffix.lower()\n",
    "    if ext in {\".csv\", \".txt\"}:\n",
    "        return pd.read_csv(p, low_memory=False, encoding=\"utf-8-sig\")\n",
    "    elif ext in {\".xlsx\", \".xls\"}:\n",
    "        return pd.read_excel(p)   \n",
    "    else:\n",
    "        raise ValueError(f\"{ext}\")\n",
    "\n",
    "\n",
    "bio      = read_any(BIO_PATH)\n",
    "env      = read_any(ENV_PATH)\n",
    "sites_df = read_any(SITES_PATH)\n",
    "\n",
    "print(\"Loaded → bio:\", bio.shape, \"| env:\", env.shape, \"| sites:\", sites_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f84a2-ab3c-4979-b454-4746e0fd2dee",
   "metadata": {},
   "source": [
    "## Step 2 – Extract canonical site list  \n",
    "This section reads the list of seagrass monitoring sites from the location file.  \n",
    "It automatically detects the correct site-name column (e.g., `survey_nam`, `site`, or `location`) and extracts all unique names.  \n",
    "Whitespace and formatting are cleaned to produce a standardized, alphabetically sorted `SITE_LIST`.  \n",
    "This canonical list is used throughout the analysis to ensure site names are consistent between biology and environmental datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7fb1218-a88d-4419-8d00-1a3f98e6b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sites: ['Abbot_Point', 'Cairns', 'Clairview', 'Gladstone', 'HayPoint', 'Mourilyan', 'OSRA', 'Townsville']\n"
     ]
    }
   ],
   "source": [
    "# Site list (from your 8-site CSV)\n",
    "# Pick the site-name column (your file looks like: name, LAT, LON)\n",
    "site_col_guess = next((c for c in sites_df.columns\n",
    "                       if str(c).lower() in {\"survey_nam\",\"site name\",\"site\",\"location\",\"name\"}),\n",
    "                      sites_df.columns[0])\n",
    "\n",
    "SITE_LIST = sorted(sites_df[site_col_guess].astype(str).str.strip().unique())\n",
    "print(\"Sites:\", SITE_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9eaaf1-a292-4993-953e-322522a78e02",
   "metadata": {},
   "source": [
    "## Step 3 – Build monthly ZC occupancy (2010–2018)\n",
    "Identify key columns in the biology table case-insensitively, then standardize site names to the canonical 8.  \n",
    "Parse months robustly (numbers, full names, 3-letter) to construct a monthly `date` (YYYY-MM-01) and filter to 2010–2018.  \n",
    "Create a `ZC_present` flag and aggregate by site×month to get n, k, p, Wilson 95% CI, and continuity-corrected `p_cc`.  \n",
    "Print year coverage and site ranges to verify parsing; preview the first rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff355f2e-643e-40aa-b721-378a5fb9dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year coverage after parsing: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]\n",
      "Rows with unparsed dates (should be 0): 0\n",
      "Occupancy years found: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>nunique</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SURVEY_NAM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abbot_Point</th>\n",
       "      <td>2010</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cairns</th>\n",
       "      <td>2010</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clairview</th>\n",
       "      <td>2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gladstone</th>\n",
       "      <td>2010</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HayPoint</th>\n",
       "      <td>2011</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mourilyan</th>\n",
       "      <td>2010</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSRA</th>\n",
       "      <td>2011</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Townsville</th>\n",
       "      <td>2010</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              min   max  nunique\n",
       "SURVEY_NAM                      \n",
       "Abbot_Point  2010  2018        9\n",
       "Cairns       2010  2018        9\n",
       "Clairview    2017  2018        2\n",
       "Gladstone    2010  2018        9\n",
       "HayPoint     2011  2018        6\n",
       "Mourilyan    2010  2018        9\n",
       "OSRA         2011  2014        4\n",
       "Townsville   2010  2018        9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SURVEY_NAM</th>\n",
       "      <th>date</th>\n",
       "      <th>n</th>\n",
       "      <th>k</th>\n",
       "      <th>p</th>\n",
       "      <th>p_lo</th>\n",
       "      <th>p_hi</th>\n",
       "      <th>p_cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124555</td>\n",
       "      <td>0.017857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>158</td>\n",
       "      <td>7</td>\n",
       "      <td>0.044304</td>\n",
       "      <td>0.021624</td>\n",
       "      <td>0.088616</td>\n",
       "      <td>0.047170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096419</td>\n",
       "      <td>0.013514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>0.036496</td>\n",
       "      <td>0.015688</td>\n",
       "      <td>0.082589</td>\n",
       "      <td>0.039855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2011-03-01</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032878</td>\n",
       "      <td>0.004386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2011-05-01</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034043</td>\n",
       "      <td>0.004545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023304</td>\n",
       "      <td>0.003086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2012-02-01</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027083</td>\n",
       "      <td>0.003597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032324</td>\n",
       "      <td>0.004310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031789</td>\n",
       "      <td>0.004237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034973</td>\n",
       "      <td>0.004673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Abbot_Point</td>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>369</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010303</td>\n",
       "      <td>0.001351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SURVEY_NAM       date    n  k         p      p_lo      p_hi      p_cc\n",
       "0   Abbot_Point 2010-02-01   27  0  0.000000  0.000000  0.124555  0.017857\n",
       "1   Abbot_Point 2010-06-01  158  7  0.044304  0.021624  0.088616  0.047170\n",
       "2   Abbot_Point 2010-11-01   36  0  0.000000  0.000000  0.096419  0.013514\n",
       "3   Abbot_Point 2010-12-01  137  5  0.036496  0.015688  0.082589  0.039855\n",
       "4   Abbot_Point 2011-03-01  113  0  0.000000  0.000000  0.032878  0.004386\n",
       "5   Abbot_Point 2011-05-01  109  0  0.000000  0.000000  0.034043  0.004545\n",
       "6   Abbot_Point 2011-09-01  161  0  0.000000  0.000000  0.023304  0.003086\n",
       "7   Abbot_Point 2012-02-01  138  0  0.000000  0.000000  0.027083  0.003597\n",
       "8   Abbot_Point 2012-06-01  115  0  0.000000  0.000000  0.032324  0.004310\n",
       "9   Abbot_Point 2012-09-01  117  0  0.000000  0.000000  0.031789  0.004237\n",
       "10  Abbot_Point 2013-01-01  106  0  0.000000  0.000000  0.034973  0.004673\n",
       "11  Abbot_Point 2013-04-01  369  0  0.000000  0.000000  0.010303  0.001351"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Rebuild monthly occupancy from raw `bio` (robust + keeps YEAR until the end) ----\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Identify key columns (case-insensitive)\n",
    "colmap = {c.lower(): c for c in bio.columns}\n",
    "\n",
    "def pick(opts):\n",
    "    for o in opts:\n",
    "        if o in colmap:\n",
    "            return colmap[o]\n",
    "    raise KeyError(f\"None of {opts} found. Available: {list(bio.columns)[:20]} ...\")\n",
    "\n",
    "site_col  = pick([\"survey_nam\",\"site name\",\"site\",\"location\"])\n",
    "year_col  = pick([\"year\"])\n",
    "month_col = pick([\"month\"])\n",
    "\n",
    "# Zostera capricorni column (e.g., 'Z_CAPRICOR')\n",
    "zcol = None\n",
    "for c in bio.columns:\n",
    "    cl = str(c).lower()\n",
    "    if cl.startswith(\"z_\") and (\"cap\" in cl or \"mueller\" in cl or \"zostera\" in cl):\n",
    "        zcol = c; break\n",
    "if zcol is None:\n",
    "    zcol = pick([\"z_capricor\"])\n",
    "\n",
    "# 2) Normalise site names to catch minor variants and keep your 8 sites\n",
    "def _norm(s): return \"\".join(ch for ch in str(s).lower() if ch.isalnum())\n",
    "site_norm_set = {_norm(s) for s in SITE_LIST}\n",
    "site_map = { _norm(s): s for s in SITE_LIST }\n",
    "\n",
    "bio_f = bio.copy()\n",
    "bio_f[\"_site_norm\"] = bio_f[site_col].apply(_norm)\n",
    "bio_f = bio_f[bio_f[\"_site_norm\"].isin(site_map)].copy()\n",
    "bio_f[\"SURVEY_NAM\"] = bio_f[\"_site_norm\"].map(site_map)\n",
    "bio_f.drop(columns=[\"_site_norm\"], inplace=True)\n",
    "\n",
    "# 3) Filter years, parse month robustly (numbers, full names, 3-letter abbr; trims spaces)\n",
    "bio_f[year_col] = pd.to_numeric(bio_f[year_col], errors=\"coerce\")\n",
    "bio_f = bio_f[bio_f[year_col].between(2010, 2018)].copy()\n",
    "\n",
    "mn = bio_f[month_col].astype(str).str.strip().str.lower()\n",
    "month_num = pd.to_numeric(mn, errors=\"coerce\")\n",
    "\n",
    "# full names then 3-letter abbreviations\n",
    "full_map = MONTH_MAP.copy()                    # from Step 0\n",
    "abbr_map = {k[:3]: v for k, v in MONTH_MAP.items()}\n",
    "mask = month_num.isna()\n",
    "month_num[mask]  = mn[mask].map(full_map)\n",
    "mask2 = month_num.isna()\n",
    "month_num[mask2] = mn[mask2].str[:3].map(abbr_map)\n",
    "\n",
    "bio_f[\"MONTH_NUM\"] = pd.to_numeric(month_num, errors=\"coerce\").astype(\"Int64\")\n",
    "bio_f[\"date\"] = pd.to_datetime(\n",
    "    dict(year=bio_f[year_col], month=bio_f[\"MONTH_NUM\"], day=1),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "print(\"Year coverage after parsing:\", sorted(bio_f[year_col].dropna().unique().tolist()))\n",
    "print(\"Rows with unparsed dates (should be 0):\", int(bio_f[\"date\"].isna().sum()))\n",
    "if bio_f[\"date\"].isna().any():\n",
    "    display(bio_f[bio_f[\"date\"].isna()][[site_col, year_col, month_col]].head(10))\n",
    "\n",
    "# 4) Z. capricorni flag and occupancy aggregation\n",
    "zc = bio_f[zcol].astype(str).str.strip().str.lower()\n",
    "bio_f[\"ZC_present\"] = zc.isin([\"yes\",\"present\",\"1\",\"true\",\"y\"])\n",
    "\n",
    "occ = (bio_f.groupby([\"SURVEY_NAM\",\"date\"], as_index=False)\n",
    "             .agg(n=(\"ZC_present\",\"size\"), k=(\"ZC_present\",\"sum\")))\n",
    "\n",
    "# p and Wilson CI (clip CI to [0,1] to avoid tiny negative due to roundoff)\n",
    "occ[\"p\"], occ[\"p_lo\"], occ[\"p_hi\"] = wilson_ci(occ[\"k\"], occ[\"n\"])\n",
    "occ[\"p_lo\"] = occ[\"p_lo\"].clip(lower=0); occ[\"p_hi\"] = occ[\"p_hi\"].clip(upper=1)\n",
    "occ[\"p_cc\"] = (occ[\"k\"] + 0.5) / (occ[\"n\"] + 1.0)\n",
    "\n",
    "# Check years now\n",
    "occ[\"year\"] = occ[\"date\"].dt.year\n",
    "print(\"Occupancy years found:\", sorted(occ[\"year\"].unique().tolist()))\n",
    "display(occ.groupby(\"SURVEY_NAM\")[\"year\"].agg([\"min\",\"max\",\"nunique\"]).sort_index())\n",
    "occ = occ.drop(columns=[\"year\"]).sort_values([\"SURVEY_NAM\",\"date\"])\n",
    "\n",
    "# Peek\n",
    "occ.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b32bfc-d768-4bf4-8cd9-e2c82472c15e",
   "metadata": {},
   "source": [
    "## Step 4 – Quality check: sampling effort and detections by site  \n",
    "This section summarizes sampling coverage and detection rates across sites.  \n",
    "It calculates the number of distinct months and years sampled, total survey counts, and total *Zostera capricorni* detections.  \n",
    "A secondary check counts how many years (2010–2018) each site recorded at least one detection.  \n",
    "These metrics help identify uneven sampling effort or data gaps before environmental modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75d7d77b-db9a-40d6-b6db-39690e72438d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             months_sampled  years_sampled  total_samples  total_Zc_hits\n",
      "SURVEY_NAM                                                              \n",
      "Abbot_Point              26              9           3590             31\n",
      "Cairns                   24              9           4250            185\n",
      "Clairview                 3              2            245             50\n",
      "Gladstone                17              9          23578           3447\n",
      "HayPoint                  9              6           1914              5\n",
      "Mourilyan                17              9           2056              0\n",
      "OSRA                      4              4           2833              7\n",
      "Townsville               12              9           6321            914\n",
      "SURVEY_NAM\n",
      "Abbot_Point    7\n",
      "Cairns         9\n",
      "Clairview      2\n",
      "Gladstone      9\n",
      "HayPoint       2\n",
      "Mourilyan      0\n",
      "OSRA           1\n",
      "Townsville     9\n",
      "Name: years_with_Zc, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# QC summary: sampling and detections by site\n",
    "occ_qc = (\n",
    "    occ.assign(year=occ[\"date\"].dt.year)\n",
    "       .groupby(\"SURVEY_NAM\")\n",
    "       .agg(\n",
    "           months_sampled = (\"date\", \"nunique\"),\n",
    "           years_sampled  = (\"year\", \"nunique\"),\n",
    "           total_samples  = (\"n\", \"sum\"),\n",
    "           total_Zc_hits  = (\"k\", \"sum\"),\n",
    "       )\n",
    "       .sort_index()\n",
    ")\n",
    "print(occ_qc)\n",
    "\n",
    "# How many years (2010–2018) each site had at least one Zc detection?\n",
    "yrs_with_zc = (\n",
    "    occ.assign(year=occ[\"date\"].dt.year)\n",
    "       .groupby([\"SURVEY_NAM\",\"year\"])[\"k\"].sum()\n",
    "       .gt(0).groupby(\"SURVEY_NAM\").sum()\n",
    "       .rename(\"years_with_Zc\")\n",
    ")\n",
    "print(yrs_with_zc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec3c842-1fa6-488b-b7e2-764f5a1acc06",
   "metadata": {},
   "source": [
    "## Step 5 – eReefs drivers: monthly panel and derived speeds  \n",
    "Convert the raw eReefs table into a tidy monthly site×variable matrix.  \n",
    "Standardize the datetime column, filter to the 8 sites and 2010–2018, then pivot `Variable` into wide columns of monthly means.  \n",
    "Derive `current_speed = √(u²+v²)` and `wind_speed = √(wspeed_u²+wspeed_v²)` when vector components are available.  \n",
    "Output is a driver panel keyed by `SURVEY_NAM` and `date` for merging with occupancy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7902fc69-844a-4bfe-9a55-02f5cddfb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot eReefs drivers and derive speeds\n",
    "env2 = env.copy()\n",
    "date_col = \"Aggregated Date/Time\" if \"Aggregated Date/Time\" in env2.columns else \"date\"\n",
    "env2[\"date\"] = pd.to_datetime(env2[date_col], errors=\"coerce\")\n",
    "\n",
    "env2 = env2[env2[\"Site Name\"].isin(occ[\"SURVEY_NAM\"].unique())]\n",
    "env2 = env2[env2[\"date\"].dt.year.between(2010, 2018)]\n",
    "\n",
    "env_w = (env2.pivot_table(index=[\"Site Name\",\"date\"], columns=\"Variable\", values=\"mean\", aggfunc=\"mean\")\n",
    "              .reset_index())\n",
    "env_w.columns.name = None\n",
    "env_w = env_w.rename(columns={\"Site Name\":\"SURVEY_NAM\"})\n",
    "\n",
    "if {\"u\",\"v\"}.issubset(env_w.columns):\n",
    "    env_w[\"current_speed\"] = np.sqrt(env_w[\"u\"]**2 + env_w[\"v\"]**2)\n",
    "if {\"wspeed_u\",\"wspeed_v\"}.issubset(env_w.columns):\n",
    "    env_w[\"wind_speed\"] = np.sqrt(env_w[\"wspeed_u\"]**2 + env_w[\"wspeed_v\"]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ca061-3c65-4064-837f-2cfe7b9bc4d3",
   "metadata": {},
   "source": [
    "## Step 6 – Merge occupancy with drivers; add seasonality and lags  \n",
    "Join the ZC occupancy table with the eReefs driver panel on site and month.  \n",
    "Encode seasonality via `sin12` and `cos12` using calendar month.  \n",
    "Create 1–3 month lags for available drivers (`temp`, `salt`, `eta`, `current_speed`, `wind_speed`) by site.  \n",
    "Add `p_cc_l1` to enable simple autoregressive baselines and SARIMAX exogenous setups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "545567f1-b109-43b1-8d1e-0fc19e27d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge + add seasonality and 1–3 month lags\n",
    "merged = occ.merge(env_w, on=[\"SURVEY_NAM\",\"date\"], how=\"left\")\n",
    "merged[\"month\"] = merged[\"date\"].dt.month\n",
    "merged[\"sin12\"] = np.sin(2*np.pi*merged[\"month\"]/12)\n",
    "merged[\"cos12\"] = np.cos(2*np.pi*merged[\"month\"]/12)\n",
    "\n",
    "driver_cols = [c for c in [\"temp\",\"salt\",\"eta\",\"current_speed\",\"wind_speed\"] if c in merged.columns]\n",
    "for v in driver_cols:\n",
    "    for L in (1,2,3):\n",
    "        merged[f\"{v}_l{L}\"] = merged.groupby(\"SURVEY_NAM\")[v].shift(L)\n",
    "\n",
    "merged[\"p_cc_l1\"] = merged.groupby(\"SURVEY_NAM\")[\"p_cc\"].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b71bfe-f919-4759-bb26-de2ee6a4bd1f",
   "metadata": {},
   "source": [
    "## Step 7 – Save cleaned outputs for analysis  \n",
    "Export the processed datasets to CSV for reuse in exploratory and modeling notebooks.  \n",
    "Three outputs are generated:  \n",
    "1. `Zc_monthly_occupancy_2010_2018.csv` – monthly ZC occupancy by site  \n",
    "2. `env_drivers_pivot_2010_2018.csv` – monthly environmental drivers in wide format  \n",
    "3. `Zc_occ_with_env_2010_2018.csv` – merged dataset with lags and seasonal terms  \n",
    "All files are saved to the working directory to ensure reproducibility across analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "441777b5-65e6-478d-962f-4501d17313ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: .\n"
     ]
    }
   ],
   "source": [
    "# Save tidy outputs (so you can use them in EDA/models)\n",
    "from pathlib import Path\n",
    "OUT = Path(\".\")\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "occ.to_csv(OUT / \"Zc_monthly_occupancy_2010_2018.csv\", index=False)\n",
    "env_w.to_csv(OUT / \"env_drivers_pivot_2010_2018.csv\", index=False)\n",
    "merged.to_csv(OUT / \"Zc_occ_with_env_2010_2018.csv\", index=False)\n",
    "print(\"Saved to:\", OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062d24a5-e2f1-4e5f-b062-adb94f917ff6",
   "metadata": {},
   "source": [
    "## Step 8 – HO pipeline: occupancy, drivers, lags, and exports (2010–2018)\n",
    "Build *Halophila ovalis* monthly occupancy as a biotic covariate, aligned to the same sites and window as ZC.  \n",
    "Load biology, environment, and site files; robustly detect columns; parse months; compute n, k, p, Wilson CI, and `p_cc`.  \n",
    "Pivot eReefs (or keep wide), derive `current_speed` and `wind_speed`, merge with HO, add 1–3 month driver lags and `p_cc_l1`, then save:\n",
    "`Ho_monthly_occupancy_2010_2018.csv` and `Ho_occ_with_env_2010_2018.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14940142-769d-4f26-ae52-3016ac4f6400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "  Ho_monthly_occupancy_2010_2018.csv\n",
      "  Ho_occ_with_env_2010_2018.csv\n",
      "Folder: C:\\Users\\F\n"
     ]
    }
   ],
   "source": [
    "# --- HO pipeline (final): same outputs, robust site column, 2010–2018 filter ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# ---------------------- Paths (relative) ----------------------\n",
    "BIO_PATH   = Path(\"GBR_NESP-TWQ-3.2.1-5.4_JCU_Seagrass_1984-2018_Site-surveys.csv\")\n",
    "ENV_PATH   = Path(\"2509.28d55d4-collected.csv\")\n",
    "SITES_PATH = Path(\"seagrass location.csv\")\n",
    "OUT        = Path(\".\")  # or Path(\"./processed\")\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2010, 2018  # required time window\n",
    "\n",
    "# ---------------------- Load ----------------------\n",
    "bio      = pd.read_csv(BIO_PATH, low_memory=False)\n",
    "env      = pd.read_csv(ENV_PATH)\n",
    "sites_df = pd.read_csv(SITES_PATH)\n",
    "\n",
    "# ---------------------- Helpers ----------------------\n",
    "MONTH_MAP = {\n",
    "    **{m.lower(): i for i, m in enumerate(\n",
    "        [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\n",
    "         \"September\",\"October\",\"November\",\"December\"], start=1)},\n",
    "    \"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"aug\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dec\":12\n",
    "}\n",
    "\n",
    "def find_col(df, candidates, contains=None):\n",
    "    \"\"\"Return the first matching column name (original casing).\"\"\"\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c and c.lower() in cols:\n",
    "            return cols[c.lower()]\n",
    "    if contains:\n",
    "        for c in df.columns:\n",
    "            if contains.lower() in c.lower():\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def parse_month_series(s):\n",
    "    \"\"\"Accept numeric, English names/abbrevs, or strings like '9月'.\"\"\"\n",
    "    s = s.astype(str).str.strip()\n",
    "    out = pd.Series(np.nan, index=s.index, dtype=\"float\")\n",
    "\n",
    "    # pure digits\n",
    "    mask_num = s.str.fullmatch(r\"\\d{1,2}\")\n",
    "    out.loc[mask_num] = s.loc[mask_num].astype(float)\n",
    "\n",
    "    # '9月' or '09月'\n",
    "    zh = s.str.extract(r\"^\\s*(\\d{1,2})\\s*月\\s*$\")[0]\n",
    "    out.loc[zh.notna()] = zh.loc[zh.notna()].astype(float)\n",
    "\n",
    "    # English names / 3-letter abbrevs\n",
    "    rest = s[(~mask_num) & (zh.isna())]\n",
    "    mapped = rest.str.lower().map(MONTH_MAP)\n",
    "    out.loc[mapped.index] = mapped\n",
    "    return out.astype(\"Int64\")\n",
    "\n",
    "Z_95 = 1.959963984540054\n",
    "def wilson_ci(k, n, z=Z_95):\n",
    "    k = np.asarray(k, dtype=float)\n",
    "    n = np.asarray(n, dtype=float)\n",
    "    p = np.divide(k, n, out=np.full_like(k, np.nan, dtype=float), where=n>0)\n",
    "    denom  = 1 + z**2/n\n",
    "    center = (p + z**2/(2*n)) / denom\n",
    "    half   = (z * np.sqrt((p*(1-p) + z**2/(4*n))/n)) / denom\n",
    "    return center, center - half, center + half\n",
    "\n",
    "# ---------------------- Site list (robust) ----------------------\n",
    "site_list_col = (find_col(sites_df,\n",
    "                          [\"Site Name\",\"Site\",\"site\",\"site_name\",\"SURVEY_NAM\",\"survey_nam\"],\n",
    "                          contains=\"site\")\n",
    "                 or sites_df.columns[0])\n",
    "SITE_LIST = (sites_df[site_list_col]\n",
    "             .dropna()\n",
    "             .astype(str).str.strip()\n",
    "             .unique()\n",
    "             .tolist())\n",
    "\n",
    "# ---------------------- 1) Biology to monthly HO occupancy ----------------------\n",
    "site_col  = find_col(bio, [\"SURVEY_NAM\",\"Site\",\"site\",\"site_name\",\"survey_name\"], contains=\"site\")\n",
    "year_col  = find_col(bio, [\"YEAR\",\"Year\",\"year\"])\n",
    "month_col = find_col(bio, [\"MN\",\"Month\",\"month\"])\n",
    "if site_col is None or year_col is None or month_col is None:\n",
    "    raise KeyError(\"Cannot find site/year/month columns in biology table.\")\n",
    "\n",
    "bio_f = bio[bio[site_col].astype(str).isin(SITE_LIST)].copy()\n",
    "\n",
    "# monthly datetime (first day of month)\n",
    "month_num = parse_month_series(bio_f[month_col])\n",
    "bio_f[\"date\"] = pd.to_datetime(\n",
    "    dict(year=bio_f[year_col].astype(int), month=month_num.astype(int), day=1),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "bio_f = bio_f.rename(columns={site_col: \"SURVEY_NAM\"}).dropna(subset=[\"date\"])\n",
    "\n",
    "# hard filter to 2010–2018\n",
    "bio_f = bio_f[bio_f[\"date\"].dt.year.between(YEAR_MIN, YEAR_MAX)]\n",
    "\n",
    "# HO presence flag\n",
    "hcol = find_col(bio_f,\n",
    "                [\"Halophila ovalis\",\"H. ovalis\",\"H_ovalis\",\"H ovalis\",\"HOVALIS\",\"HO\",\"ho\"],\n",
    "                contains=\"ovalis\")\n",
    "if hcol is None:\n",
    "    raise KeyError(\"Cannot find the Halophila ovalis (HO) column in biology table.\")\n",
    "\n",
    "val = bio_f[hcol]\n",
    "if np.issubdtype(val.dtype, np.number):\n",
    "    bio_f[\"HO_present\"] = val.fillna(0) > 0\n",
    "else:\n",
    "    s = val.astype(str).str.strip().str.lower()\n",
    "    bio_f[\"HO_present\"] = s.isin([\"yes\",\"present\",\"1\",\"true\",\"y\"])\n",
    "\n",
    "# occupancy (n, k) + Wilson CI + continuity-corrected p_cc\n",
    "occ_ho = (bio_f\n",
    "          .groupby([\"SURVEY_NAM\",\"date\"], as_index=False)\n",
    "          .agg(n=(\"HO_present\",\"size\"), k=(\"HO_present\",\"sum\")))\n",
    "occ_ho[\"p\"], occ_ho[\"p_lo\"], occ_ho[\"p_hi\"] = wilson_ci(occ_ho[\"k\"], occ_ho[\"n\"])\n",
    "occ_ho[\"p_lo\"] = occ_ho[\"p_lo\"].clip(lower=0)\n",
    "occ_ho[\"p_hi\"] = occ_ho[\"p_hi\"].clip(upper=1)\n",
    "occ_ho[\"p_cc\"]  = (occ_ho[\"k\"] + 0.5) / (occ_ho[\"n\"] + 1.0)\n",
    "\n",
    "# ---------------------- 2) Environment (pivot if long) ----------------------\n",
    "env2 = env.copy()\n",
    "\n",
    "# date column\n",
    "date_col = ( \"Aggregated Date/Time\" if \"Aggregated Date/Time\" in env2.columns else\n",
    "             (find_col(env2, [\"date\",\"Date\",\"datetime\",\"Datetime\"], contains=\"date\")) )\n",
    "if date_col is None:\n",
    "    raise KeyError(\"Cannot locate a datetime column in env table (expected 'Aggregated Date/Time' or 'date').\")\n",
    "env2[\"date\"] = pd.to_datetime(env2[date_col], errors=\"coerce\")\n",
    "\n",
    "# site column\n",
    "site_env_col = ( \"Site Name\" if \"Site Name\" in env2.columns else\n",
    "                 (find_col(env2, [\"SURVEY_NAM\",\"Site\",\"site\",\"site_name\"], contains=\"site\")) )\n",
    "if site_env_col is None:\n",
    "    raise KeyError(\"Cannot find site column in env table (e.g. 'Site Name'/'site').\")\n",
    "env2 = env2.rename(columns={site_env_col: \"SURVEY_NAM\"}).dropna(subset=[\"date\"])\n",
    "\n",
    "# align to sites in biology and years 2010–2018\n",
    "env2 = env2[env2[\"SURVEY_NAM\"].astype(str).isin(occ_ho[\"SURVEY_NAM\"].astype(str).unique())]\n",
    "env2 = env2[env2[\"date\"].dt.year.between(YEAR_MIN, YEAR_MAX)]\n",
    "\n",
    "# pivot if long-form (Variable, mean), else keep as wide-form\n",
    "if {\"Variable\",\"mean\"}.issubset(env2.columns):\n",
    "    env_w = (env2.pivot_table(index=[\"SURVEY_NAM\",\"date\"],\n",
    "                              columns=\"Variable\", values=\"mean\", aggfunc=\"mean\")\n",
    "                  .reset_index())\n",
    "    env_w.columns.name = None\n",
    "else:\n",
    "    env_w = env2.copy()\n",
    "\n",
    "# derived speeds\n",
    "if {\"u\",\"v\"}.issubset(env_w.columns):\n",
    "    env_w[\"current_speed\"] = np.sqrt(env_w[\"u\"]**2 + env_w[\"v\"]**2)\n",
    "if {\"wspeed_u\",\"wspeed_v\"}.issubset(env_w.columns):\n",
    "    env_w[\"wind_speed\"] = np.sqrt(env_w[\"wspeed_u\"]**2 + env_w[\"wspeed_v\"]**2)\n",
    "\n",
    "# ---------------------- 3) Merge + lags ----------------------\n",
    "merged_ho = (occ_ho\n",
    "             .merge(env_w, on=[\"SURVEY_NAM\",\"date\"], how=\"left\")\n",
    "             .sort_values([\"SURVEY_NAM\",\"date\"]))\n",
    "\n",
    "merged_ho[\"month\"] = merged_ho[\"date\"].dt.month\n",
    "\n",
    "driver_cols = [c for c in [\"temp\",\"salt\",\"eta\",\"current_speed\",\"wind_speed\"] if c in merged_ho.columns]\n",
    "for v in driver_cols:\n",
    "    for L in (1,2,3):\n",
    "        merged_ho[f\"{v}_l{L}\"] = merged_ho.groupby(\"SURVEY_NAM\", sort=False)[v].shift(L)\n",
    "\n",
    "merged_ho[\"p_cc_l1\"] = merged_ho.groupby(\"SURVEY_NAM\", sort=False)[\"p_cc\"].shift(1)\n",
    "\n",
    "# hard filter again (safety) to 2010–2018 after merge\n",
    "merged_ho = merged_ho[merged_ho[\"date\"].dt.year.between(YEAR_MIN, YEAR_MAX)]\n",
    "\n",
    "# ---------------------- 4) Save ----------------------\n",
    "occ_ho.sort_values([\"SURVEY_NAM\",\"date\"]).to_csv(OUT / \"Ho_monthly_occupancy_2010_2018.csv\", index=False)\n",
    "merged_ho.to_csv(OUT / \"Ho_occ_with_env_2010_2018.csv\", index=False)\n",
    "print(\"Saved:\\n  Ho_monthly_occupancy_2010_2018.csv\\n  Ho_occ_with_env_2010_2018.csv\\nFolder:\", OUT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115256ae-a603-4e71-81a4-f8601bf0f98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
